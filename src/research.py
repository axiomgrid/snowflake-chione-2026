"""
RESEARCH.PY — MAP-Elites Shape Exploration Engine
==================================================
Intelligently explores the (alpha, gamma, beta, steps) parameter space
of the Reiter snowflake model to discover diverse morphologies.

Uses MAP-Elites (Quality-Diversity) algorithm:
- Maintains a 2D behavioral map indexed by (CompactnessNormalized, BranchingFactor)
- Each cell stores the best-performing (alpha, gamma, beta, steps) for that shape niche
- New candidates are generated by mutating existing elites
- Goal: fill every niche with a distinct snowflake morphology
"""
import os
import json
import time
import csv
import re
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

from .utils import calculate_metrics, get_ice_cmap


# =============================================================================
# MAP-Elites Archive
# =============================================================================

class MAPElitesArchive:
    """2D behavioral map for snowflake shape diversity."""
    
    def __init__(self, resolution=10):
        self.resolution = resolution
        # Map cells: keyed by (row, col) -> dict with params, metrics, fitness
        self.cells = {}
        # Behavioral descriptor ranges
        # Axis 1: CompactnessNormalized (0-1)
        # Axis 2: BranchingFactor (0 - max_observed)
        self.bf_max = 50.0  # Initial estimate, will grow
    
    def get_cell(self, compactness_norm, branching_factor):
        """Map behavioral descriptors to grid cell indices."""
        r = int(np.clip(compactness_norm * self.resolution, 0, self.resolution - 1))
        # Normalize branching factor to [0, 1] using current max
        bf_norm = np.clip(branching_factor / self.bf_max, 0.0, 1.0)
        c = int(np.clip(bf_norm * self.resolution, 0, self.resolution - 1))
        return (r, c)
    
    def try_insert(self, params, metrics, fitness):
        """Insert into map if cell is empty or fitness is better."""
        cn = metrics.get('compactness_normalized', 0)
        bf = metrics.get('branching_factor', 0)
        
        # Update branching factor max if needed
        if bf > self.bf_max:
            self.bf_max = bf * 1.2  # 20% headroom
        
        cell = self.get_cell(cn, bf)
        
        entry = {
            'params': params,
            'metrics': metrics,
            'fitness': fitness,
            'cell': cell
        }
        
        if cell not in self.cells or fitness > self.cells[cell]['fitness']:
            self.cells[cell] = entry
            return True  # Inserted
        return False  # Rejected
    
    def get_random_elite(self):
        """Select a random elite from the archive for mutation."""
        if not self.cells:
            return None
        key = list(self.cells.keys())[np.random.randint(len(self.cells))]
        return self.cells[key]
    
    def coverage(self):
        """Fraction of cells filled."""
        return len(self.cells) / (self.resolution ** 2)
    
    def save(self, path):
        """Save archive state to JSON."""
        data = {
            'resolution': self.resolution,
            'bf_max': self.bf_max,
            'cells': {}
        }
        for k, v in self.cells.items():
            key_str = f"{k[0]},{k[1]}"
            data['cells'][key_str] = {
                'params': v['params'],
                'metrics': v['metrics'],
                'fitness': v['fitness']
            }
        with open(path, 'w') as f:
            json.dump(data, f, indent=2)
    
    @classmethod
    def load(cls, path):
        """Load archive state from JSON."""
        with open(path, 'r') as f:
            data = json.load(f)
        archive = cls(resolution=data['resolution'])
        archive.bf_max = data.get('bf_max', 50.0)
        for key_str, v in data['cells'].items():
            r, c = key_str.split(',')
            cell = (int(r), int(c))
            archive.cells[cell] = {
                'params': v['params'],
                'metrics': v['metrics'],
                'fitness': v['fitness'],
                'cell': cell
            }
        return archive


# =============================================================================
# Parameter Mutation
# =============================================================================

# Expanded ranges per the research plan
PARAM_RANGES = {
    'alpha': (0.001, 5.0),
    'gamma': (0.0, 0.05),
    'beta':  (0.1, 0.9),
    'steps': (100, 2000)
}

def mutate_params(params, sigma=0.15):
    """
    Gaussian mutation on log-scale for alpha/gamma, linear for beta.
    Discrete mutation for steps.
    10% chance of random restart (uniform sample from full range).
    """
    if np.random.random() < 0.10:
        # Random restart
        return {
            'alpha': np.exp(np.random.uniform(np.log(PARAM_RANGES['alpha'][0]),
                                                np.log(PARAM_RANGES['alpha'][1]))),
            'gamma': np.random.uniform(*PARAM_RANGES['gamma']),
            'beta':  np.random.uniform(*PARAM_RANGES['beta']),
            'steps': int(np.random.uniform(*PARAM_RANGES['steps']))
        }
    
    new = {}
    
    # Alpha: log-space mutation
    log_a = np.log(max(params['alpha'], 1e-6))
    log_a += np.random.normal(0, sigma)
    new['alpha'] = np.clip(np.exp(log_a), *PARAM_RANGES['alpha'])
    
    # Gamma: log-space mutation
    if params['gamma'] < 1e-6:
        new['gamma'] = np.random.uniform(0, 0.001)
    else:
        log_g = np.log(max(params['gamma'], 1e-8))
        log_g += np.random.normal(0, sigma)
        new['gamma'] = np.clip(np.exp(log_g), *PARAM_RANGES['gamma'])
    
    # Beta: linear mutation
    new['beta'] = np.clip(params['beta'] + np.random.normal(0, 0.05),
                          *PARAM_RANGES['beta'])
    
    # Steps: discrete mutation
    # ±50 to ±200 steps change
    delta_s = int(np.random.choice([-1, 1]) * np.random.randint(50, 201))
    new['steps'] = int(np.clip(params.get('steps', 600) + delta_s,
                              *PARAM_RANGES['steps']))
    
    return new


def classify_shape(area, ratio, compactness_normalized, branching_factor):
    """Classify snowflake shape based on metrics."""
    if area < 50:
        return "No Growth"
    elif compactness_normalized > 0.7 and branching_factor < 5:
        return "Plate (Hexagon)"
    elif compactness_normalized > 0.5 and branching_factor < 10:
        return "Compact Plate"
    elif compactness_normalized < 0.15 and branching_factor > 20:
        return "Dendrite (Star)"
    elif compactness_normalized < 0.3 and branching_factor > 15:
        return "Branched Dendrite"
    elif ratio > 0.6:
        return "Faceted Crystal"
    elif ratio < 0.3:
        return "Sparse Dendrite"
    else:
        return "Hybrid / Transition"


def describe_shape(params, metrics, shape_class):
    """Generate a text description of a snowflake shape for analysis files."""
    lines = []
    lines.append(f"=== Shape Analysis ===")
    lines.append(f"Alpha:   {params['alpha']:.4f}")
    lines.append(f"Gamma:   {params['gamma']:.6f}")
    lines.append(f"Beta:    {params['beta']:.4f}")
    lines.append(f"Steps (Target): {params.get('steps', 'N/A')}")
    lines.append(f"")
    lines.append(f"--- Metrics ---")
    lines.append(f"Area:                  {metrics['area']}")
    lines.append(f"Perimeter:             {metrics['perimeter']}")
    lines.append(f"Ratio (A/πR²):         {metrics['ratio']:.4f}")
    lines.append(f"Compactness (P²/A):    {metrics['compactness']:.2f}")
    lines.append(f"CompactnessNorm (4πA/P²): {metrics['compactness_normalized']:.4f}")
    lines.append(f"Max Radius:            {metrics['max_radius']:.2f}")
    lines.append(f"Branching Factor:      {metrics['branching_factor']:.2f}")
    lines.append(f"Steps (Actual):        {metrics['steps']}")
    lines.append(f"Growth Rate:           {metrics['growth_rate']:.4f}")
    lines.append(f"")
    lines.append(f"--- Classification ---")
    lines.append(f"Shape Class: {shape_class}")
    lines.append(f"")
    lines.append(f"--- Notes ---")
    
    # Auto-generated analysis based on metrics
    if metrics['compactness_normalized'] > 0.6:
        lines.append(f"• Highly compact shape — close to ideal hexagonal.")
    elif metrics['compactness_normalized'] < 0.2:
        lines.append(f"• Very fractal/branched boundary — dendritic growth dominant.")
    else:
        lines.append(f"• Moderate complexity — transitional morphology.")
    
    if metrics['branching_factor'] > 25:
        lines.append(f"• Extreme branching detected — highly intricate arms.")
    elif metrics['branching_factor'] < 5:
        lines.append(f"• Minimal branching — smooth faceted edges.")
    
    if params['beta'] < 0.2:
        lines.append(f"• Low beta (humidity) — sparse growth environment.")
    elif params['beta'] > 0.7:
        lines.append(f"• High beta (humidity) — vigorous growth expected.")
    
    if params['gamma'] < 0.0005:
        lines.append(f"• Very low gamma — tendency toward simple plate morphology.")
    elif params['gamma'] > 0.02:
        lines.append(f"• High gamma — fast vapor addition, complex growth likely.")
    
    if params.get('steps', 0) < 300:
         lines.append(f"• Short simulation run — preventing boundary saturation.")
    
    return "\n".join(lines)


def load_plan_file(path):
    """
    Load targeted parameter combinations from a text file.
    Expected format lines: alpha=0.1 gamma=0.01 beta=0.4 steps=600 ...
    Returns list of dicts.
    """
    candidates = []
    if not path or not os.path.exists(path):
        return candidates
    
    with open(path, 'r') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            # Simple regex parsing
            try:
                # Extract values using regex to handle spacing variations
                a = float(re.search(r'alpha=([\d\.]+)', line).group(1))
                g = float(re.search(r'gamma=([\d\.]+)', line).group(1))
                b = float(re.search(r'beta=([\d\.]+)', line).group(1))
                s_match = re.search(r'steps=([\d]+)', line)
                s = int(s_match.group(1)) if s_match else 600
                
                candidates.append({
                    'alpha': a, 'gamma': g, 'beta': b, 'steps': s
                })
            except Exception as e:
                print(f"[Research] Warning: Failed to parse plan line '{line}': {e}")
                
    print(f"[Research] Loaded {len(candidates)} candidates from plan file {path}")
    return candidates


# =============================================================================
# Main Research Runner
# =============================================================================

def run_shape_research(given_dirs, output_dir, plan_file=None, budget=50, 
                       map_resolution=10, default_steps=600, use_gpu=False, force_cpu=False):
    """
    Main entry point for shape exploration research.
    """
    from .engine import run_single_sim, CUDA_AVAILABLE
    if use_gpu and not CUDA_AVAILABLE:
        use_gpu = False
    if force_cpu:
        use_gpu = False
    
    # --- Setup Output Directory ---
    epoch = int(time.time())
    if output_dir:
        run_name = f"Run_v39_research_shapes_{output_dir}"
    else:
        run_name = f"Run_v39_research_shapes_{epoch}"
    
    run_dir = os.path.join("results", run_name, "research_shapes")
    img_dir = os.path.join(run_dir, "Intermediate")
    data_dir = os.path.join(run_dir, "grid_data")
    analysis_dir = os.path.join(run_dir, "analysis")
    parent_dir = os.path.join("results", run_name)
    
    os.makedirs(img_dir, exist_ok=True)
    os.makedirs(data_dir, exist_ok=True)
    os.makedirs(analysis_dir, exist_ok=True)
    
    archive_path = os.path.join(parent_dir, "map_archive.json")
    csv_path = os.path.join(parent_dir, "scan_results.csv")
    
    # --- Load or Create Archive ---
    if os.path.exists(archive_path):
        archive = MAPElitesArchive.load(archive_path)
        print(f"[Research] Loaded existing archive: {len(archive.cells)} elites, {archive.coverage()*100:.1f}% coverage")
    else:
        archive = MAPElitesArchive(resolution=map_resolution)
        print(f"[Research] Created new archive: {map_resolution}x{map_resolution} = {map_resolution**2} niches")
    
    # --- Load Explored Set (skip duplicates) ---
    explored = set()
    if os.path.exists(csv_path):
        with open(csv_path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Key now includes steps (defaults to 600 if missing)
                # Note: older CSVs might have 'Steps' meaning actual steps taken, not target steps.
                # Only new CSVs with 'TargetSteps' column would suffice, but we can approximate.
                # Actually, check filenames if possible?
                # For now, simplistic approach: (a, g, b, target_steps)
                # But loading old data is tricky.
                # Let's assume old data means target_steps=600.
                a = float(row['Alpha'])
                g = float(row['Gamma'])
                b = float(row.get('Beta', 0.4))
                # Distinguish between actual steps taken (row['Steps']) and target (param)
                # Assuming 600 for old data. For new data, we should save 'TargetSteps'.
                t_steps = int(row.get('TargetSteps', 600))
                
                key = (a, g, b, t_steps)
                explored.add(key)
        print(f"[Research] {len(explored)} previously explored combinations loaded")
    
    # --- Seed from Given Directories ---
    seed_count = 0
    for gdir in given_dirs:
        seed_csv = os.path.join(gdir, "scan_results.csv")
        if not os.path.exists(seed_csv): continue
        
        with open(seed_csv, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                a = float(row['Alpha'])
                g = float(row['Gamma'])
                b = 0.4
                t_steps = 600 # Legacy runs
                
                area = float(row.get('Area', 0))
                perim = float(row.get('Perimeter', 0))
                ratio_val = float(row.get('Ratio', 0))
                comp = float(row.get('Compactness', 0))
                radius = float(row.get('Radius', 0))
                steps_taken = int(float(row.get('Steps', 0)))
                gr = float(row.get('GrowthRate', 0))
                cn = float(row.get('CompactnessNormalized', 0))
                bf = float(row.get('BranchingFactor', 0))
                
                if cn == 0 and perim > 0 and area > 0:
                    cn = min(1.0, max(0.0, (4 * np.pi * area) / (perim**2 + 1e-6)))
                if bf == 0 and perim > 0 and radius > 0:
                    bf = perim / (2 * np.pi * radius + 1e-6)
                
                metrics = {
                    'area': area, 'perimeter': perim, 'ratio': ratio_val,
                    'compactness': comp, 'max_radius': radius,
                    'compactness_normalized': cn, 'branching_factor': bf,
                    'steps': steps_taken, 'growth_rate': gr
                }
                fitness = area * (1 + steps_taken / 1000.0)
                
                archive.try_insert({'alpha': a, 'gamma': g, 'beta': b, 'steps': t_steps}, 
                                   metrics, fitness)
                seed_count += 1
    
    # --- Load Plan Candidates ---
    plan_candidates = load_plan_file(plan_file)
    # Filter out already explored plan candidates
    plan_candidates = [c for c in plan_candidates 
                       if (c['alpha'], c['gamma'], c['beta'], c['steps']) not in explored]
    if plan_candidates:
        print(f"[Research] {len(plan_candidates)} new candidates from plan file entering queue.")
    
    # --- MAP-Elites Exploration Loop ---
    print(f"\n{'='*60}")
    print(f"  MAP-Elites Shape Exploration — Budget: {budget}")
    print(f"  Plan File: {os.path.basename(plan_file) if plan_file else 'None'} ({len(plan_candidates)} queue)")
    print(f"{'='*60}\n")
    
    new_results = []
    new_count = 0
    cmap = get_ice_cmap()
    
    for i in range(budget):
        # Selection Strategy: Plan -> Mutation -> Random
        if plan_candidates:
            # Consume from plan queue
            candidate = plan_candidates.pop(0)
            source = "PLAN"
        elif archive.cells and np.random.random() > 0.3:
            # Mutate existing elite
            elite = archive.get_random_elite()
            candidate = mutate_params(elite['params'])
            source = "MUTATION"
        else:
            # Random sample
            candidate = {
                'alpha': np.exp(np.random.uniform(np.log(PARAM_RANGES['alpha'][0]),
                                                    np.log(PARAM_RANGES['alpha'][1]))),
                'gamma': np.random.uniform(*PARAM_RANGES['gamma']),
                'beta':  np.random.uniform(*PARAM_RANGES['beta']),
                'steps': int(np.random.uniform(*PARAM_RANGES['steps'])) 
            }
            source = "RANDOM"
        
        a, g, b, st = candidate['alpha'], candidate['gamma'], candidate['beta'], candidate.get('steps', 600)
        
        # Rounding
        a = round(a, 4); g = round(g, 6); b = round(b, 4); st = int(st)
        candidate = {'alpha': a, 'gamma': g, 'beta': b, 'steps': st}
        
        # Check cache
        key = (a, g, b, st)
        if key in explored:
            continue
        explored.add(key)
        
        param_str = f"Alpha{a:.4f}_Gamma{g:.6f}_Beta{b:.4f}_Steps{st}"
        npz_path = os.path.join(data_dir, f"data_{param_str}.npz")
        csv_grid_path = os.path.join(data_dir, f"grid_{param_str}.csv")
        img_path = os.path.join(img_dir, f"snowflake_{param_str}.png")
        analysis_path = os.path.join(analysis_dir, f"shape_{param_str}.txt")
        
        if os.path.exists(npz_path):
            try:
                data = np.load(npz_path)
                final_grid = data['final']
                freeze_grid = data['freeze']
                print(f"  [{i+1}/{budget}] [Cache] {param_str}")
            except:
                continue
        else:
            print(f"  [{i+1}/{budget}] [{source}] α={a:.4f} γ={g:.6f} β={b:.4f} steps={st} ...", end="", flush=True)
            t0 = time.time()
            if use_gpu:
                from .engine import run_single_sim_gpu
                final_grid, freeze_grid, _ = run_single_sim_gpu(
                    a, g, b, steps=st, stop_at_edge=False, return_history=False
                )
            else:
                final_grid, freeze_grid, _ = run_single_sim(
                    a, g, b, steps=st, stop_at_edge=False, return_history=False
                )
            dt = time.time() - t0
            print(f" done in {dt:.1f}s")
            
            np.savez_compressed(npz_path, final=final_grid, freeze=freeze_grid)
            np.savetxt(csv_grid_path, final_grid, delimiter=",", fmt='%.4f')

        # Metrics & Archive Update
        area, perim, ratio_val, comp, max_radius, cn, bf = calculate_metrics(final_grid)
        
        steps_taken = 0
        if np.any(freeze_grid > 0):
            steps_taken = int(np.max(freeze_grid))
        growth_rate = area / max(1, steps_taken)
        shape_class = classify_shape(area, ratio_val, cn, bf)
        
        metrics = {
            'area': int(area), 'perimeter': int(perim), 'ratio': ratio_val,
            'compactness': comp, 'max_radius': max_radius,
            'compactness_normalized': cn, 'branching_factor': bf,
            'steps': steps_taken, 'growth_rate': growth_rate
        }
        fitness = area * (1 + steps_taken / 1000.0)
        
        inserted = archive.try_insert(candidate, metrics, fitness)
        status = "NEW ELITE" if inserted else "rejected"
        print(f"         -> {shape_class} | CN={cn:.3f} BF={bf:.1f} | {status}")
        
        # Image gen
        if not os.path.exists(img_path):
            rows, cols = final_grid.shape
            cr = rows // 2
            half = min(150, cr)
            view = final_grid[cr-half:cr+half+1, cr-half:cr+half+1]
            fig, ax = plt.subplots(1, 1, figsize=(6, 6), dpi=150)
            ax.imshow(view, cmap=cmap, vmin=0, vmax=1.2, interpolation='nearest')
            ax.set_title(f"α={a:.4f} γ={g:.6f} β={b:.4f} steps={st}\n{shape_class}", fontsize=10)
            ax.axis('off')
            fig.tight_layout()
            fig.savefig(img_path, dpi=150, bbox_inches='tight')
            plt.close(fig)
            
        with open(analysis_path, 'w') as f:
            f.write(describe_shape(candidate, metrics, shape_class))
            
        new_results.append({
            'Alpha': a, 'Gamma': g, 'Beta': b, 'TargetSteps': st,
            'Area': int(area), 'Perimeter': int(perim),
            'Ratio': round(ratio_val, 6), 'Compactness': round(comp, 4),
            'Radius': round(max_radius, 4),
            'Steps': steps_taken, 'GrowthRate': round(growth_rate, 4),
            'Class': shape_class, 'Video': '',
            'CompactnessNormalized': round(cn, 6),
            'BranchingFactor': round(bf, 4),
            'Temperature': '', 'Supersaturation': ''
        })
        new_count += 1

    # --- Save CSV ---
    csv_columns = ['Alpha', 'Gamma', 'Beta', 'TargetSteps', 'Area', 'Perimeter', 'Ratio',
                   'Compactness', 'Radius', 'Steps', 'GrowthRate', 'Class',
                   'Video', 'CompactnessNormalized', 'BranchingFactor',
                   'Temperature', 'Supersaturation']
    
    write_header = not os.path.exists(csv_path)
    # Check if existing CSV has TargetSteps column; if not, we are modifying schema!
    # Simple workaround: just append. 
    # If header doesn't exist, we write it. If it does, we assume it's okay or we just append rows.
    # To be safe, if file exists but lacks TargetSteps, we can't easily fix it without rewriting.
    # We'll just append and let CSV readers handle the extra column or missing header.
    
    with open(csv_path, 'a', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=csv_columns)
        if write_header:
            writer.writeheader()
        for r in new_results:
            writer.writerow(r)
            
    archive.save(archive_path)
    
    # --- Summary ---
    summary_path = os.path.join(parent_dir, f"summary_report_{epoch}.txt")
    with open(summary_path, 'w') as f:
        f.write(f"=== MAP-Elites Shape Exploration Report ===\n")
        f.write(f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Budget: {budget} | New Shapes: {new_count}\n")
        f.write(f"Archive: {len(archive.cells)}/{archive.resolution**2} niches ({archive.coverage()*100:.1f}%)\n")
        if plan_file:
            f.write(f"Plan File: {os.path.basename(plan_file)}\n")
        
        if new_results:
            f.write(f"\n--- Top 5 Interesting Shapes ---\n")
            mean_cn = np.mean([r['CompactnessNormalized'] for r in new_results])
            scored = [(r, abs(r['CompactnessNormalized'] - mean_cn) * r['Area'] * r['BranchingFactor']) 
                      for r in new_results]
            scored.sort(key=lambda x: -x[1])
            for r, score in scored[:5]:
                f.write(f"  α={r['Alpha']:.4f} γ={r['Gamma']:.6f} β={r['Beta']:.4f} steps={r['TargetSteps']}"
                        f" -> {r['Class']} (CN={r['CompactnessNormalized']:.3f})\n")
    
    print(f"\n{'='*60}")
    print(f"  Research Complete! New: {new_count}")
    print(f"  Report: {summary_path}")
    print(f"{'='*60}")
    
    return new_count, archive
